{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "# 1. Import the Necessary Packages #\n",
    "# -------------------------------- #\n",
    "#import gym\n",
    "from unityagents import UnityEnvironment\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ #\n",
    "# 2. Instantiate the Environment #\n",
    "# ------------------------------ #\n",
    "# environment with 1 Agent\n",
    "env1 = UnityEnvironment(file_name='./Reacher_1.app')\n",
    "# get the default brain for env1\n",
    "brain1_name = env1.brain_names[0]\n",
    "brain1 = env1.brains[brain1_name]\n",
    "\n",
    "# environment with 20 Agent\n",
    "#env2 = UnityEnvironment(file_name='./Reacher_20.app')\n",
    "# get the default brain for env20\n",
    "#brain20_name = env20.brain_names[0]\n",
    "#brain20 = env20.brains[brain20_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agent: 1\n",
      "Size of each action: 4\n",
      "There are 1 agent. It observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------- #\n",
    "# 3. Examine the State and Action Spaces #\n",
    "# -------------------------------------- #\n",
    "# reset the environment\n",
    "env1_info = env1.reset(train_mode=True)[brain1_name]\n",
    "\n",
    "# number of agents\n",
    "num_agent1 = len(env1_info.agents)\n",
    "print('Number of agent:', num_agent1)\n",
    "\n",
    "# size of each action\n",
    "action_size1 = brain1.vector_action_space_size\n",
    "print('Size of each action:', action_size1)\n",
    "\n",
    "# examine the state space \n",
    "states1 = env1_info.vector_observations\n",
    "state_size1 = states1.shape[1]\n",
    "print('There are {} agent. It observes a state with length: {}'.format(states1.shape[0], state_size1))\n",
    "print('The state for the first agent looks like:', states1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = env1_info.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_train(env, brain_name,\n",
    "               n_episodes=1000, max_t=300, print_every=100):\n",
    "    \"\"\"Train Agent(s) with Deep Deterministic Policy Gradients.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        env: the environment to train agent(s)\n",
    "        brain_name: the default brain name of the environment\n",
    "        n_episodes (int): total episodes\n",
    "        max_t (int): maximum time steps for each episode\n",
    "        print_every (int): episode interval to print the scores outcome\n",
    "    \"\"\"\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    # number of agents\n",
    "    n_agents = len(env_info.agents)\n",
    "    n_scores_deque = [deque(maxlen=print_every) for i in range(len)]\n",
    "    n_scores = []\n",
    "\n",
    "    action_size = env.brains[brain_name].vector_action_space_size\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # get the current state (for each agent)\n",
    "        states = env_info.vector_observations\n",
    "        # initialize the score (for each agent)\n",
    "        scores = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            # select an action (for each agent)\n",
    "            actions = np.random.randn(len(env_info.agents), action_size)\n",
    "            # send all actions to the environment\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            # get next state (for each agent)\n",
    "            next_states = env_info.vector_observations\n",
    "            # get reward (for each agent)\n",
    "            rewards = env_info.rewards\n",
    "            # see if episode finished\n",
    "            dones = env_info.done\n",
    "            # update the score (for each agent)\n",
    "            scores += env_info.rewards\n",
    "            # roll over states to next time step\n",
    "            states = next_states\n",
    "            # exit loop if episode finished\n",
    "            if done:\n",
    "                break \n",
    "        # reset the environment \n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        # reset all agents \n",
    "        agents.reset()\n",
    "                \n",
    "        n_scores_deque.append(scores)\n",
    "        n_scores.append(scores)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2005f2701dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m scores1 = ddpg(env=env1, agent=agent1,\n\u001b[0;32m----> 2\u001b[0;31m                n_episodes=100, max_t=20, print_every=10)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ecece87028b0>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(env, agent, n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DRL_projects/Projects/p2_continuous-control/ddpg_agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, add_noise)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got dict)"
     ]
    }
   ],
   "source": [
    "scores1 = ddpg(env=env1, agent=agent1,\n",
    "               n_episodes=100, max_t=20, print_every=10)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report Catalog**\n",
    "\n",
    "- 1. Learning Algorithm\n",
    "    - 1.1 Basic Concepts\n",
    "    - 1.2 Policy Gradient Methods\n",
    "    - 1.3 PPO\n",
    "    - 1.4 A3C\n",
    "    - 1.5 DDPG\n",
    "    - 1.6 D4PG\n",
    "- 2. Plot of Rewards\n",
    "- 3. Ideas of Future Work\n",
    "\n",
    "\n",
    "# 1. Learning Algorithm\n",
    "Try Policy-Based method from simple to complex.\n",
    "\n",
    "## 1.1 Basic Concepts \n",
    "Define the __policy neural network__.\n",
    "In Policy-Based Methods, the agent use a neural network to aproximate either a stochastic or a deterministic policy.\n",
    "Here're some basic concepts of Policy:\n",
    "\n",
    "### On-Policy & Off-Policy\n",
    "\n",
    "- __On-Policy: π(collecting experience) = π(trained)__\n",
    "    \n",
    "    The policy π(collecting experience) used for interacting with the environment is ALSO the policy π(trained).\n",
    "\n",
    "\n",
    "- __Off-Policy: π(collecting experience) ≠ π(trained)__\n",
    "\n",
    "    The policy π(collecting experience) used for interacting with the environment is NOT the policy π(trained). \n",
    "        - π(collecting experience) is ε-Greedy policy \n",
    "        - while π(trained)is the optimal policy.\n",
    "    \n",
    "### Stochastic Policy & Deterministic Policy\n",
    "\n",
    "- __Stochastic Policy__ wants to learn the Probability Distribution over the actions.\n",
    "- __Deterministic Policy__ beleives that the best action every single time when we inquire the Actor (Policy) nerual network. Thus it always outputs the best believed action for any given state.\n",
    "\n",
    "### Value-Based and Policy-Based\n",
    "\n",
    "- __Value-Based Methods__: the agent uses its experience with the environment to maintain an estimate of the optimal action-value function. The optimal policy is then obtained from the optimal action-value function estimate.\n",
    "```\n",
    "class Value_Based_Network():\n",
    "    def __init__():\n",
    "        ...    \n",
    "    def forward():\n",
    "        ...\n",
    "```\n",
    "- __Policy-Based Methods__: directly learn the optimal policy, without having to maintain a separate value function estimate.\n",
    "```\n",
    "class Policy_Based_Network():\n",
    "    def __init__():\n",
    "        ...   \n",
    "    def forward():\n",
    "        ...    \n",
    "    def act():\n",
    "        ...\n",
    "```\n",
    "\n",
    "## 1.2 Policy Gradient Methods\n",
    "\n",
    "### 1.2 .1 Hill Climbing \n",
    "Pseudocode\n",
    "<img src=\"CC_imgs/HillClimbing.png\">\n",
    "\n",
    "NOTE: __Episode Return G__ vs. __Expected Return J__\n",
    "\n",
    "Due to randomness in the environment(and the policy, if it is stochastic), it is highly like that if we collect a second episode with the same values for θ, we'll likely get a different value for the return G.\n",
    "The (sampled) return **G** is not a perfect but good enough __estimate for the expected return J__.\n",
    "\n",
    "### 1.2.2 REINFORCE\n",
    "<img src=\"./readme_imgs/REINFORCE.png\">\n",
    "\n",
    "## 1.3 [PPO](https://arxiv.org/pdf/1707.06347.pdf)(Proximal Policy Optimization Algorithms)\n",
    "\n",
    "### 1.3.1 The Surrogate Function\n",
    "\n",
    "The __importance sampling__ below tells us we can use old trajectories for computing averages for new policy, as long as we add this extra re-weighting factor, that takes into account how under or over–represented each trajectory is under the new policy compared to the old one.\n",
    "<img src=\"./readme_imgs/importance_sampling.png\">\n",
    "\n",
    "Expanding the __re-weighting factor__:\n",
    "<img src=\"./readme_imgs/re-weighting_factor.png\">\n",
    "\n",
    "The approximate form of the gradient, we can think of it as the gradient of a new object, called the __surrogate function__\n",
    "<img src=\"./readme_imgs/surrogate_function.png\">\n",
    "So using this new gradient, we can perform gradient ascent to update our policy -- which can be thought as directly maximize the surrogate function.\n",
    "\n",
    "### 1.3.2 Clipping Policy Updates\n",
    "\n",
    "implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```\n",
    "\n",
    "### 1.3.3 PPO Summary\n",
    "\n",
    "PPO(Proximal Policy Optimization) algorithm:\n",
    "<img src=\"./readme_imgs/PPO_summary.png\">\n",
    "\n",
    "## 1.4 [A3C](https://arxiv.org/pdf/1602.01783.pdf)(Asynchronous Advantage Actor-critic)\n",
    "\n",
    "\n",
    "## 1.5 DDPG(Deep Deterministic Policy Gradient)\n",
    "\n",
    "## 1.6 [D4PG](https://openreview.net/pdf?id=SyZipzbCb)(Distributed Distributional Deterministic Policy)\n",
    "\n",
    "[Reference](https://github.com/ShangtongZhang/DeepRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot of Rewards\n",
    "A plot of rewards per episode is included to illustrate that either:\n",
    "\n",
    "- __version 1__ the agent receives an average reward (over 100 episodes) of at least +30, or\n",
    "![1Agent_Plot]<img src=\"./readme_imgs/1Agent_Plot.png\">\n",
    "\n",
    "- __version 2__ the agent is able to receive an average reward (over 100 episodes, and over all 20 agents) of at least +30.\n",
    "![20Agents_Plot]<img src=\"./readme_imgs/20Agents_Plot.png\">\n",
    "\n",
    "\n",
    "# 3. Ideas of Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
